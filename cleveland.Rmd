---
title: "Cleveland classifier"
author: "Eleanor Barnes"
date: "28th November 2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r installations, echo = T, results = 'hide', warning=F, message = F}
library(tidyverse)
library(xgboost)
library(caret)
library(Hmisc)
library(e1071)
library(glmnet)
library(pROC)


set.seed(100)
```

## Aim:
Supervised statistical learning
to identify the combination of features that are more likely to be associated with heart disease.

Using R version 4.0.3 (2020-10-10)

### 1. Functions

Here, I've written some functions for later use in the code. These are a normalisation function & an XGBoost grid search cross validation function. The normalisation function takes vector/columns as inputs. 

```{r functions}
normalise = function(x){
  normalised = (x - min(x))/(max(x)-min(x))
}

xgb_cv = function(train_data, train_label, k = 5){

grid_cv <- expand.grid( eta = c(0.01, 0.001, 0.0001),
                        max_depth = c(4,6,8,10), 
                        gamma=c(0:5), 
                        subsample=seq(0.5,1,by = 0.1)
)
best_params = data.frame(eta = NA, max_depth = NA, gamma = NA,
                         subsample = NA, accuracy =NA, k = NA)
for(n in 1:k){
  set.seed(n*1000)
  train_index= sample(nrow(train_data),floor(0.75*nrow(train_data)))
  train_data = as.matrix(train_data[train_index,])
  validation_data = as.matrix(train_data[-train_index,])
  
  train_label = train_label[train_index]
  validation_label = train_label[-train_index]
  
  #
  xgb_train = xgb.DMatrix(data=train_data,label=train_label)

  
  all_accuracy = vector(length = nrow(grid_cv))
  
  for(i in 1:nrow(grid_cv)){
  params = list(
    booster="gbtree",
    eta = grid_cv$eta[i],
    max_depth = grid_cv$max_depth[i],
    gamma = grid_cv$gamma[i],
    subsample = grid_cv$subsample[i],
    objective ="binary:logistic",
    eval_metric ="logloss"
  )
  
  
  xgb_fit=xgb.train(
    params=params,
    data=xgb_train,
    nrounds=10000,
    early_stopping_rounds=10,
    watchlist=list(val1=xgb_train,val2=xgb_test),
    verbose=0
  )

  #Evaluating model
  
  xgb_validation_predictions = round(predict(xgb_fit,validation_data,
                                             reshape=T))
  xgb_validation_accuracy = sum(xgb_validation_predictions==validation_label)/
    length(xgb_validation_predictions)
  print(as.character(i))
  all_accuracy[i] = xgb_validation_accuracy
  }
  best_params[n,] = cbind(grid_cv[which.max(all_accuracy),], accuracy = max(all_accuracy), k = n)
  
}

  return(best_params)
}
```

## 2. Loading the dataset
### 2.1. Load heart disease dataset
```{r dataload}

raw_data = read.csv(
  'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data', 
  header = F)
str(raw_data) #Ensuring all rows/columns have loaded correctly
colnames(raw_data) = c("age", "sex", "cp", "trestbps", "chol", "fbs",
                       "restecg", "thalach", "exang", "oldpeak", "slope",
                       "ca","thal", "outcome")
raw_data$ca = as.numeric(raw_data$ca)
raw_data$thal = as.numeric(raw_data$thal)

```

303 observations & 14 variables have been loaded, but without column names, which have been given above. Likewise, 2 columns were loaded as characters, and have been changed. 

### 2.2. Data structures

Checking dataset and preprocessing odd structures. NAs introduced due to numeric conversion, they are missing values and have been removed. 

```{r structures}
sum(is.na(raw_data))
anyDuplicated(raw_data)

data = na.omit(raw_data) 


```
No duplicated rows were identified. 

### 2.3. Exploring the data

```{r explore, warning = F, message = F}
###Exploring the dataset----

ggplot(pivot_longer(data, 1:13, names_to = "metric", values_to = "value"),
       aes(x = value)) + 
  geom_histogram()+ 
  facet_wrap(~metric, scales = "free")+
  labs(title = 'Distribution of features')

ggplot(data, aes(x = outcome)) +geom_histogram() + 
  labs(title = 'Distribution of outcome')

summary(data)
```
There is a mixture of categorical and continuous predictors.  

## 3. XGBoost: Binary Outcome

## 3.1. Preprocessing

The outcome is converted into binary: has heart disease (1) or doesn't have heart disease (0). xgboost requires the one-hot encoding of categorical variables. 
```{r ppxgb}

binary_data = data%>%
  mutate(outcome = case_when(outcome>0 ~ 1, TRUE ~ 0))%>% 
  mutate(thal = as.character(thal), 
         cp =as.character(cp),
         slope = as.character(slope) 
         )

#One-hot encoding all categorical predictor variables. 
#Else, XGBoost preserves numeric values 
dummification = dummyVars(outcome ~., binary_data)
dummy_data = data.frame(predict(dummification, binary_data))

label = as.integer(binary_data$outcome)


```


### 3.2 Train test split

75%-25% split selected. xgboost package requires data in matrix format. 
```{r ttsxgb}
#Train-test split
train_index= sample(nrow(dummy_data),floor(0.75*nrow(dummy_data)))
train_data = as.matrix(dummy_data[train_index,])
test_data = as.matrix(dummy_data[-train_index,])

train_label = label[train_index]
test_label = label[-train_index]

xgb_train = xgb.DMatrix(data=train_data,label=train_label)
xgb_test = xgb.DMatrix(data=test_data,label=test_label)

```

### 3.3. Cross validating hyperparameters
10-fold cross validation would enable tuning of model, but has not been evaluated due to time/computation limits. xgboost doesn't include a grid search cross validator so one is written in this code (function xgb_cv()).
```{r cvxgb, eval = FALSE}

#Cross validating hyperparameters
highest_performers = xgb_cv(train_data, train_label, k=10) %>%
  group_by(eta, max_depth, gamma, subsample)%>%
  summarise(mean = mean(accuracy, na.rm), count = n()) %>%
  filter(max(count))
cv_params = list(
  booster="gbtree",
  eta=highest_performers$eta,
  max_depth=highest_performers$max_depth,
  gamma=highest_performers$gamma,
  subsample=highest_performers$subsample,
  colsample_bytree=1,
  objective="binary:logistic",
  eval_metric="logloss"
)


```

### 3.4. Set parameters
Though the previous chunk includes a cross validation step, this hasn't been evaluated due to time restrictions. Instead, I've drawn on these parameters from an online source (listed in my report). 

```{r paramxgb1}
from_internet_params = list(
  booster="gbtree",
  eta=0.1,
  max_depth=4,
  gamma=3,
  subsample=0.5,
  colsample_bytree=1,
  objective="binary:logistic",
  eval_metric="logloss"
)
```


### 3.5 Train model
```{r trainxgb}
#Train model
binary_xgb_fit=xgb.train(
  params=from_internet_params,
  data=xgb_train,
  nrounds=10000,
  early_stopping_rounds=10,
  watchlist=list(val1=xgb_train,val2=xgb_test),
  verbose=0
)
```


### 3.6 Understanding feature importance

```{r featuresxgb}
#Plotting feature importance
binary_feature_importance <- xgb.importance(
  feature_names = colnames(dummy_data),
  model = binary_xgb_fit)
xgb.plot.importance(importance_matrix = binary_feature_importance) 

```
cp4, ca, age & thal3 seem to be important features. 

### 3.7 Evaluation of model:
```{r evaluatexgb}
#Evaluating model
binary_xgb_train_predictions = round(predict(binary_xgb_fit,
                                             train_data,reshape=T))
binary_xgb_train_accuracy = caret::confusionMatrix(as.factor(
  binary_xgb_train_predictions), as.factor(train_label))

binary_xgb_test_predictions = round(predict(binary_xgb_fit,
                                            test_data,reshape=T))
binary_xgb_test_accuracy = caret::confusionMatrix(as.factor(
  binary_xgb_test_predictions), as.factor(test_label))
caret::confusionMatrix(as.factor(binary_xgb_test_predictions), 
                       as.factor(test_label))
```

Accuracy on test data: 84%. 

## 4. Logistic regression
### 4.1. Train-test split
Data is converted into binary, split and then normalised between 0-1.
```{r ttslr}
lr_data = data%>%
  mutate(outcome = case_when(outcome>0 ~ 1, TRUE ~ 0))

#Train-test split, Normalising data for lasso regularisation
train_index= sample(nrow(lr_data),floor(0.75*nrow(lr_data)))
train_data = lr_data[train_index,] %>% mutate(age = normalise(age), 
                    trestbps = normalise(trestbps), 
                    chol = normalise(chol), 
                    thalach = normalise(thalach), 
                    oldpeak = normalise(oldpeak), 
                    ca = normalise(ca))
test_data = lr_data[-train_index,] %>% mutate(age = normalise(age), 
                    trestbps = normalise(trestbps), 
                    chol = normalise(chol), 
                    thalach = normalise(thalach), 
                    oldpeak = normalise(oldpeak), 
                    ca = normalise(ca))
```

### 4.2. Train model
A full predictive model is run. 

```{r trainlr}
#Running a full predictive model
lr_full_model = glm(outcome ~., data = train_data, 
                    family = "binomial"(link ='logit')) 
summary(lr_full_model)

```
ca & thal are highly significant predictors. cp, trestbps, chol and slope are also significant, alongside sex, thalach and exang.

```{r anova}

anova(lr_full_model, test = 'Chisq')

```

ANOVA identifies a range of features with a statistically significant impact on heart disease status. fbs has been continually a poor predictor of outcome across a variety of methods used so far. 


### 4.3. Step AIC
Step AIC model likewise shows a set of features important to the logistic regressor model in predicting heart disease status. 
```{r steplr}
null = glm(outcome ~1, data = train_data, 
           family = "binomial"(link ='logit')) 
step = MASS::stepAIC(null, scope=list(lower=.~1,
                                      upper = formula(lr_full_model)), 
                     direction = 'both')
lr_step_model = glm(step$formula, data = train_data, 
                    family = "binomial"(link ='logit')) 
step$formula
```

### 4.4. Evaluation

```{r evallr}
#Evaluating models
lr_predictions_train =  round(predict(lr_full_model,
                                      newdata = train_data, type = 'response'))
lr_predictions_test =  round(predict(lr_full_model, 
                                     newdata = test_data, type = 'response'))

lr_accuracy = caret::confusionMatrix(as.factor(
  as.numeric(lr_predictions_test)), as.factor(test_data$outcome))
caret::confusionMatrix(as.factor(as.numeric(lr_predictions_test)),
                       as.factor(test_data$outcome))
```
A logistic regression has a test accuracy of 76%. 


## 5. Logistic regression LASSO regularisation

### 5.1. Setting up data
glmnet requires data in matrix format. 
```{r lrldata}
#Lasso regularisation
lasso_train_data = as.matrix(select(train_data, -outcome))
lasso_train_label = as.matrix(train_data$outcome)

lasso_test_data = as.matrix(select(test_data, -outcome))
lasso_test_label = as.matrix(test_data$outcome)

```

### 5.2. Cross validate lambda parameter
Lambda weights are a hyperparameter and are thus cross validated directly in glmnet. 
```{r cvlrl}
#Cross validate for lambda value
lambdas = 10^seq(-2,2, 0.1)
cv_lasso = cv.glmnet(lasso_train_data, lasso_train_label, 
                     alpha = 1, family = binomial(link = 'logit'), 
                     lambda = lambdas)
cv_lasso$lambda.min
```

### 5.3. Train logistic regressor
Logistic regressor trained using previously identified lamba value (0.01). 
```{r lrltrain}
#Train model
lr_lasso_model = glmnet(lasso_train_data, lasso_train_label, 
                        alpha = 1, family = binomial(link = 'logit'), 
                        lambda = cv_lasso$lambda.min)

```

### 5.4. Coefficients of features
```{r coeflrl}
#Coefficients of features
coef(lr_lasso_model)

```
This shows trestbps, chol, thalach and ca as important features. Age is again shown to be less important as a predictor. 

### 5.5. Evaluate model
```{r evallrl}
#Evaluate model
train_predictions = round(predict(lr_lasso_model, 
                                  lasso_train_data, 
                                  s = cv_lasso$lambda.min, 
                                  type = 'response'))
test_predictions = round(predict(lr_lasso_model, 
                                 lasso_test_data,
                                 s = cv_lasso$lambda.min, 
                                 type = 'response'))

lasso_accuracy = caret::confusionMatrix(as.factor(as.numeric(
  test_predictions)), as.factor(lasso_test_label))
caret::confusionMatrix(as.factor(as.numeric(test_predictions)), 
                       as.factor(lasso_test_label))
```
LASSO accuracy on test data was 74.7%, but with a wide range of uncertainty. 

## 6. Best performing model
```{r best}
print(paste0('Accuracy on test data...Lasso:',
             as.character(lasso_accuracy$overall[1]), 
             ' Logistic Regression:', 
  as.character(lr_accuracy$overall[1]), ' XGBoost: ', 
  as.character(binary_xgb_test_accuracy$overall[1])))
```

XGBoost was the best performing model at 84% accuracy. 
